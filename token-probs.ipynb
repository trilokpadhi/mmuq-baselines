{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: es are a great way\n",
      "\n",
      "Token\t| Next Token\t| Log Probability\n",
      "--------------------------------------------------\n",
      "es\t|  are\t | -2.0427\n",
      " are\t|  a\t | -2.3210\n",
      " a\t|  great\t | -2.9873\n",
      " great\t|  way\t | -3.1264\n",
      "\n",
      "Total log probability of the generated sequence: -12.4331\n",
      "Total probability of the generated sequence: 3.9843e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import math\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prepare input\n",
    "prompt = \"The quick brown fox\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate tokens\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=5,  # Generate 5 new tokens\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True  # To get the logits for each token\n",
    ")\n",
    "\n",
    "# Compute transition scores (log probabilities of the generated tokens)\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")\n",
    "\n",
    "# Get the generated tokens (excluding the prompt tokens)\n",
    "generated_tokens = outputs.sequences[0, input_ids.shape[1]:]\n",
    "\n",
    "# Print the generated sequence\n",
    "print(f\"Generated sequence: {tokenizer.decode(generated_tokens)}\")\n",
    "\n",
    "# Display transition scores for each token\n",
    "print(\"\\nToken\\t| Next Token\\t| Log Probability\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(len(generated_tokens) - 1):\n",
    "    current_token = tokenizer.decode(generated_tokens[i])\n",
    "    next_token = tokenizer.decode(generated_tokens[i + 1])\n",
    "    log_prob = transition_scores[0, i].item()\n",
    "    print(f\"{current_token}\\t| {next_token}\\t | {log_prob:.4f}\")\n",
    "\n",
    "# Calculate sequence log probability\n",
    "sequence_log_prob = transition_scores[0].sum().item()\n",
    "sequence_prob = math.exp(sequence_log_prob)\n",
    "\n",
    "print(f\"\\nTotal log probability of the generated sequence: {sequence_log_prob:.4f}\")\n",
    "print(f\"Total probability of the generated sequence: {sequence_prob:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
