{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpadhi1/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/generic.py:339: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world's greatest artists were all born to be masters of their craft. The first to be born in a world of music, the first to be born to be an artist, and the last to be born in the world of art.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"gpt2\"  # Replace with your model # what is the model id for gpt2? Answer: gpt2 is the model id for GPT-2 ? ANswr \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the prompt\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Set generation parameters\n",
    "max_new_tokens = 50\n",
    "temperature = 1.0\n",
    "top_k = 3\n",
    "top_p = 0.95\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Initialize the generated sequence with the input prompt\n",
    "generated_ids = input_ids\n",
    "\n",
    "# Autoregressive generation loop\n",
    "for _ in range(max_new_tokens):\n",
    "    # Forward pass to get logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=generated_ids)\n",
    "        logits = outputs.logits[:, -1, :]  # Get logits for the last generated token\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Apply top-k filtering\n",
    "    if top_k > 0:\n",
    "        top_k_values, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "        logits[logits < top_k_values[:, -1, None]] = -float('Inf')\n",
    "\n",
    "    # Apply top-p (nucleus) filtering\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "        sorted_indices_to_remove[:, 0] = 0\n",
    "        for batch_idx in range(logits.size(0)):\n",
    "            indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
    "            logits[batch_idx, indices_to_remove] = -float('Inf')\n",
    "\n",
    "    # Convert logits to probabilities and sample the next token\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "    # Append the sampled token to the generated sequence\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "\n",
    "    # Check for end-of-sequence token\n",
    "    if next_token_id.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "# Decode the generated sequence into text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf61858e032474a94acb01c8f581a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedf38a51d1d40d192ef243e30068274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label  1 5 represents the L ava C ave feature in the graph ic illustr ation . n , indicating the location of a large under ground passage filled with mol ten la va , as part of an active vol cano . This cave can be used \n",
      "\n",
      "Generated Output:\n",
      " USER:   What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud ASSISTANT: The label 15 represents the Lava Cave feature in the graphic illustration.n, indicating the location of a large underground passage filled with molten lava, as part of an active volcano. This cave can be used\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and processor\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"  # Replace with the correct LLaVA model\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# Prompt and image input\n",
    "# image_path = \"path_to_your_image.jpg\"  # Replace with your image path\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "prompt = \"<s> USER: <image> What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud ASSISTANT:\"\n",
    "\n",
    "# Preprocess image and text\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "input_ids = inputs[\"input_ids\"]  # Initial input IDs\n",
    "pixel_values = inputs[\"pixel_values\"]  # Image pixel values\n",
    "\n",
    "# Set generation parameters\n",
    "max_new_tokens = 50\n",
    "temperature = 1.0\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "eos_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "# Initialize generated_ids\n",
    "generated_ids = input_ids\n",
    "\n",
    "# Autoregressive decoding loop\n",
    "for _ in range(max_new_tokens):\n",
    "    # Forward pass: Pass the updated input_ids and static pixel_values\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=generated_ids, pixel_values=pixel_values)\n",
    "\n",
    "    # Get logits for the last token\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Top-k filtering\n",
    "    if top_k > 0:\n",
    "        top_k_values, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "        logits[logits < top_k_values[:, -1, None]] = -float('Inf')\n",
    "\n",
    "    # Top-p (nucleus) filtering\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "        sorted_indices_to_remove[:, 0] = 0\n",
    "        for batch_idx in range(logits.size(0)):\n",
    "            indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]\n",
    "            logits[batch_idx, indices_to_remove] = -float('Inf')\n",
    "\n",
    "    # Sample the next token\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "    # Append the sampled token to generated_ids\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "\n",
    "    # Decode and print the generated token\n",
    "    decoded_token = processor.tokenizer.decode(next_token_id.item(), skip_special_tokens=True)\n",
    "    print(decoded_token, end=\" \", flush=True)\n",
    "\n",
    "    # Break if EOS token is generated\n",
    "    if next_token_id.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "# Final output decoding\n",
    "generated_text = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"\\n\\nGenerated Output:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpadhi1/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " USER:   What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud ASSISTANT: The label 15 represents the lava flow.\n",
      "\n",
      "In the image, there is a diagram of a mountain with a lava flow, and the number 15 is placed near the lava flow. This number likely represents the depth\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 50\n",
    "temperature = 1.0\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
